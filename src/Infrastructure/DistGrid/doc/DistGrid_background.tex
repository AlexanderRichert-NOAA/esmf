% $Id: DistGrid_background.tex,v 1.6 2002/05/07 03:38:46 vbalaji Exp $

\section{Background}

Scalable implementations of finite-difference codes are generally
based on decomposing the model domain into subdomains that are
distributed among processors. These domains may then be obliged to
share data at their boundaries if data dependencies are merely
local, or may need to acquire information from the global domain if
there are extended data dependencies, as in the spectral transform, or
in elliptic solvers. The \emph{distributed grid}, or
\textbf{DistGrid}, is a category within ESMF used for expressing, and
performing operations that involve, dependencies among data
distributed across processors.

\subsection{Scope}

The discrete representation of data fields within a model component
begins with the definition of \emph{physical grid} associated with the
data. The data fields thus get defined as arrays, which are then
distributed among processors. The indices associated with array
locations in each dimension thus define a global \emph{index space}. The
\textbf{DistGrid} encompasses all the ESMF infrastructure operations
associated with the index-space representation of data. Operations
requiring knowledge of actual physical locations and distances between
locations belong to the \textbf{PhysGrid}, and are found in the
documents associated with it.

\subsection{Location}

The \textbf{GriddedComponent}, \textbf{PhysGrid} and \textbf{DistGrid}
form a closely-linked conceptual chain. The \textbf{DistGrid} also
provides the interface to the machine layer where the scheduling,
communication and memory management primitives reside. Applications
should rarely need to reach beyond the \textbf{DistGrid} layer for
direct invocation of the communication primitives.


\subsection{Summary}

A gridded component is associated with one or more global physical
grids.  Distributing a global physical grid across PEs generates a
\textbf{PhysGrid}, which is associated with a a single
\textbf{DistGrid}, distributed across some or all of the PEs
associated with the component. The \textbf{DistGrid} has operations to
define domain decompositions on the pelist, and given user-specified
data dependencies, it can define topologies, i.e. connectivities on
the pelist for scheduling communication. It contains all the
operations for sharing data according to those dependencies.  This
includes familiar operations like the halo update and data transpose.
It can perform global reductions (sum, max, min, maxloc, minloc) on
distributed data.  The sum has a bitwise exact option. It can create a
copy of the global data on one or more PEs, and scatter a global array
across a PElist.

The \textbf{DistGrid} will perform sign flips, vector component interchanges,
and redundancy checks as needed on certain types of grids (e.g
tripolar grid and cubed-sphere).

Operations within \textbf{DistGrid} do not include those which are
potentially dependent on grid metrics. For instance, certain kinds of
averaging operations are extremely simple on regular grids. These are
metric-dependent on irregularly spaced grids, and are hence considered
to belong to the \textbf{PhysGrid}.

